{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e4bc8ae",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b013c6",
   "metadata": {},
   "source": [
    "We see 10 labels with 50000 cases and 10000 test cases. With an even spread of classes within the test cases. We need to change the data for it to be able to form an image and we needed to create a function to unpickle all the data for it to be preprocessed to feed into our model.\n",
    "\n",
    "From printing the first 5 from each label we see it is low quality but our images are formed correctly.\n",
    "\n",
    "While creating our train and test data sets we take 5000 of our label and 5000 randomly sampled for the train set. Then 1000 from our label and 1000 randomly sampled for the test set . After this we encode our labels for binary input into our model. 0 if frog not in image and 1 if frog is in image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a8440",
   "metadata": {},
   "source": [
    "# Which label you picked for item/animal detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c88fa0",
   "metadata": {},
   "source": [
    "I chose frog. Label y = 6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da3bc08",
   "metadata": {},
   "source": [
    "## Optional: Why?\n",
    "Because it's funny but also interesting looking at the first 5 model images and also frogs are popular from memes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad0b04",
   "metadata": {},
   "source": [
    "## Modeling Process\n",
    "## What is the shape of your model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0de05",
   "metadata": {},
   "source": [
    "My model has 4 convolution Layers, 3 batch_normalizations and 2 dense layers.\n",
    "\n",
    "In Order:\n",
    "conv2d (Conv2D) a 2D Convolution Layer, this layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs\n",
    "\n",
    "batch_normalization  (Batch normalization is a layer that allows every layer of the network to do learning more independently)\n",
    "\n",
    "conv2d_1 (Conv2D)\n",
    "\n",
    "batch_normalization_1\n",
    "\n",
    "conv2d_2\n",
    "\n",
    "batch_normalization_2\n",
    "\n",
    "conv2d_3             \n",
    " batch_normalization_3                                                                                                   \n",
    "Dense (Dense is the only actual network layer in that model. A Dense layer feeds all outputs from the previous layer to all its neurons, each neuron providing one output to the next layer)\n",
    "\n",
    "dense_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423431d2",
   "metadata": {},
   "source": [
    "## Why did you pick selected loss and optimizer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e5c8e",
   "metadata": {},
   "source": [
    "The optimizer I have chosen is SGD (Stochastic gradient descent). I have chosen this because it is very fast, robust, and flexible. The only problem would be when we are stuck at local minima whenever we deal with large multi-dimensional datasets. But since we arenâ€™t using large multi-dimensional datasets it is ok.\n",
    "\n",
    "The loss function I have chosen computes the cross entropy loss between the labels and predictions. This means more weight is given to outcomes that are unexpected by my model, meaning it will learn more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2c15a2",
   "metadata": {},
   "source": [
    "# Model Performance\n",
    "\n",
    "## Show the results of your training through the use of graphs. \n",
    "## The graphs can be obtained from tensorboard. These include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fafb0e8",
   "metadata": {},
   "source": [
    "## Accuracy :\n",
    "\n",
    "We can see that initially the accuracy starts off very high and only continues to go up over the time of our epochs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab91b7d",
   "metadata": {},
   "source": [
    "## Loss:\n",
    "\n",
    "We see that our loss is consistently going down. This is good as it means that for each individual image it predicts on, it becomes more correct and less wrong in its prediction. We could have gone over more epochs to see the loss plateau off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45acdb6",
   "metadata": {},
   "source": [
    "## Validation Accuracy:\n",
    "    \n",
    "It starts low and increases and stabilizes around 90%. This means that around 90% of the time our model will predict the image given correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a17e0c",
   "metadata": {},
   "source": [
    "## Validation Loss:\n",
    "The decreasing of the validation loss as well as loss means that the model is still getting better over each epoch and that we cut it off too early. This means my model still has room for growth and can be better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27beae0",
   "metadata": {},
   "source": [
    "## Conclusion.\n",
    "We can see that from the graphs of validation loss, loss, accuracy and validation accuracy, my model had a validation accuracy of around 90% with a validation loss and loss still decreasing towards 0. This meant that my model could've been better given more epochs. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
